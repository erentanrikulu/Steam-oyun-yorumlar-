{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74a34015-36c2-4c06-972f-cb7c6687d583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\eren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\eren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 1 elements, new values have 2 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m df_stemmed \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/eren/Desktop/sıkıldım1/data/stemmed_sentences.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Sütun adlarını düzelt ve temizle\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m df_lemmatized\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     30\u001b[0m df_lemmatized \u001b[38;5;241m=\u001b[39m df_lemmatized\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m     31\u001b[0m df_lemmatized \u001b[38;5;241m=\u001b[39m df_lemmatized[df_lemmatized[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:6313\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   6311\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   6312\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n\u001b[1;32m-> 6313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value)\n\u001b[0;32m   6314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m   6315\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mproperties.pyx:69\u001b[0m, in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:814\u001b[0m, in \u001b[0;36mNDFrame._set_axis\u001b[1;34m(self, axis, labels)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;124;03mThis is called from the cython code when we set the `index` attribute\u001b[39;00m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;124;03mdirectly, e.g. `series.index = [1, 2, 3]`.\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    813\u001b[0m labels \u001b[38;5;241m=\u001b[39m ensure_index(labels)\n\u001b[1;32m--> 814\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mset_axis(axis, labels)\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:238\u001b[0m, in \u001b[0;36mBaseBlockManager.set_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: AxisInt, new_labels: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;66;03m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_set_axis(axis, new_labels)\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis] \u001b[38;5;241m=\u001b[39m new_labels\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\base.py:98\u001b[0m, in \u001b[0;36mDataManager._validate_set_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m new_len \u001b[38;5;241m!=\u001b[39m old_len:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength mismatch: Expected axis has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements, new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length mismatch: Expected axis has 1 elements, new values have 2 elements"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import load_npz\n",
    "from itertools import combinations\n",
    "import pickle\n",
    "\n",
    "# NLTK verilerini indir\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Tokenizasyon fonksiyonu\n",
    "def proper_tokenize(text):\n",
    "    text = re.sub(r'[^a-zA-ZğüşıöçĞÜŞİÖÇ\\s]', '', text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word not in stop_words and len(word) > 1]\n",
    "\n",
    "# Veri setlerini yükle\n",
    "df_lemmatized = pd.read_csv(\"C:/Users/eren/Desktop/sıkıldım1/data/lemmatized_sentences.csv\")\n",
    "df_stemmed = pd.read_csv(\"C:/Users/eren/Desktop/sıkıldım1/data/stemmed_sentences.csv\")\n",
    "\n",
    "# Sütun adlarını düzelt ve temizle\n",
    "df_lemmatized.columns = [\"document_id\", \"content\"]\n",
    "df_lemmatized = df_lemmatized.dropna()\n",
    "df_lemmatized = df_lemmatized[df_lemmatized[\"content\"].str.strip() != \"\"]\n",
    "df_lemmatized['tokens'] = df_lemmatized['content'].apply(proper_tokenize)\n",
    "\n",
    "df_stemmed.columns = [\"document_id\", \"content\"]\n",
    "df_stemmed = df_stemmed.dropna()\n",
    "df_stemmed = df_stemmed[df_stemmed[\"content\"].str.strip() != \"\"]\n",
    "df_stemmed['tokens'] = df_stemmed['content'].apply(proper_tokenize)\n",
    "\n",
    "# Giriş metnini seç (veri setinden rastgele bir yorum)\n",
    "input_text = df_lemmatized['content'].iloc[0]  # Ör. \"This game is so fun and engaging for solo players\"\n",
    "input_tokens = proper_tokenize(input_text)\n",
    "\n",
    "# TF-IDF modellerini ve vektörlerini yükle\n",
    "tfidf_lemmatized_matrix = load_npz(\"tfidf_lemmatized.npz\")  # TF-IDF vektör matrisi\n",
    "tfidf_stemmed_matrix = load_npz(\"tfidf_stemmed.npz\")\n",
    "with open(\"tfidf_lemmatized_model.pkl\", \"rb\") as f:\n",
    "    tfidf_lemmatized = pickle.load(f)\n",
    "with open(\"tfidf_stemmed_model.pkl\", \"rb\") as f:\n",
    "    tfidf_stemmed = pickle.load(f)\n",
    "\n",
    "# Word2Vec modellerini yükle\n",
    "word2vec_models = {\n",
    "    \"lemmatized_cbow_vs100_w2\": Word2Vec.load(\"lemmatized_model_cbow_vs100_w2.model\"),\n",
    "    \"lemmatized_cbow_vs100_w4\": Word2Vec.load(\"lemmatized_model_cbow_vs100_w4.model\"),\n",
    "    \"lemmatized_cbow_vs300_w2\": Word2Vec.load(\"lemmatized_model_cbow_vs300_w2.model\"),\n",
    "    \"lemmatized_cbow_vs300_w4\": Word2Vec.load(\"lemmatized_model_cbow_vs300_w4.model\"),\n",
    "    \"lemmatized_skipgram_vs100_w2\": Word2Vec.load(\"lemmatized_model_skipgram_vs100_w2.model\"),\n",
    "    \"lemmatized_skipgram_vs100_w4\": Word2Vec.load(\"lemmatized_model_skipgram_vs100_w4.model\"),\n",
    "    \"lemmatized_skipgram_vs300_w2\": Word2Vec.load(\"lemmatized_model_skipgram_vs300_w2.model\"),\n",
    "    \"lemmatized_skipgram_vs300_w4\": Word2Vec.load(\"lemmatized_model_skipgram_vs300_w4.model\"),\n",
    "    \"stemmed_cbow_vs100_w2\": Word2Vec.load(\"stemmed_model_cbow_vs100_w2.model\"),\n",
    "    \"stemmed_cbow_vs100_w4\": Word2Vec.load(\"stemmed_model_cbow_vs100_w4.model\"),\n",
    "    \"stemmed_cbow_vs300_w2\": Word2Vec.load(\"stemmed_model_cbow_vs300_w2.model\"),\n",
    "    \"stemmed_cbow_vs300_w4\": Word2Vec.load(\"stemmed_model_cbow_vs300_w4.model\"),\n",
    "    \"stemmed_skipgram_vs100_w2\": Word2Vec.load(\"stemmed_model_skipgram_vs100_w2.model\"),\n",
    "    \"stemmed_skipgram_vs100_w4\": Word2Vec.load(\"stemmed_model_skipgram_vs100_w4.model\"),\n",
    "    \"stemmed_skipgram_vs300_w2\": Word2Vec.load(\"stemmed_model_skipgram_vs300_w2.model\"),\n",
    "    \"stemmed_skipgram_vs300_w4\": Word2Vec.load(\"stemmed_model_skipgram_vs300_w4.model\")\n",
    "}\n",
    "\n",
    "# Cümle vektörü oluştur\n",
    "def get_sentence_vector(sentence, model):\n",
    "    vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "# TF-IDF için giriş metni vektörü\n",
    "def get_tfidf_vector(text, tfidf_model, tfidf_vectorizer):\n",
    "    return tfidf_vectorizer.transform([text]).toarray()\n",
    "\n",
    "# Benzerlik hesaplama fonksiyonu\n",
    "def find_top_5_similar(model_name, input_vector, vectors, df, is_tfidf=False):\n",
    "    if is_tfidf:\n",
    "        similarities = cosine_similarity(input_vector, vectors)[0]\n",
    "    else:\n",
    "        similarities = cosine_similarity([input_vector], vectors)[0]\n",
    "    top_5_indices = np.argsort(similarities)[-5:][::-1]\n",
    "    results = []\n",
    "    for idx in top_5_indices:\n",
    "        results.append({\n",
    "            \"document_id\": df['document_id'].iloc[idx],\n",
    "            \"content\": df['content'].iloc[idx],\n",
    "            \"similarity_score\": similarities[idx]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Tüm modeller için sonuçları topla\n",
    "all_results = {}\n",
    "\n",
    "# TF-IDF Benzerlik\n",
    "input_tfidf_lemmatized = get_tfidf_vector(input_text, tfidf_lemmatized, tfidf_lemmatized)\n",
    "all_results[\"tfidf_lemmatized\"] = find_top_5_similar(\"tfidf_lemmatized\", input_tfidf_lemmatized, tfidf_lemmatized_matrix, df_lemmatized, is_tfidf=True)\n",
    "\n",
    "input_tfidf_stemmed = get_tfidf_vector(input_text, tfidf_stemmed, tfidf_stemmed)\n",
    "all_results[\"tfidf_stemmed\"] = find_top_5_similar(\"tfidf_stemmed\", input_tfidf_stemmed, tfidf_stemmed_matrix, df_stemmed, is_tfidf=True)\n",
    "\n",
    "# Word2Vec Benzerlik\n",
    "for model_name, model in word2vec_models.items():\n",
    "    # Veri setine göre doğru df seç\n",
    "    df = df_lemmatized if \"lemmatized\" in model_name else df_stemmed\n",
    "    # Cümle vektörlerini hazırla\n",
    "    sentence_vectors = [get_sentence_vector(tokens, model) for tokens in df['tokens']]\n",
    "    # Giriş metni vektörü\n",
    "    input_vector = get_sentence_vector(input_tokens, model)\n",
    "    all_results[model_name] = find_top_5_similar(model_name, input_vector, sentence_vectors, df)\n",
    "\n",
    "# Anlamsal Değerlendirme (Örnek puanlar, senin incelemen gerekecek)\n",
    "semantic_scores = {}\n",
    "for model_name, results in all_results.items():\n",
    "    # Örnek puanlar (1-5), gerçekte senin değerlendirmen lazım\n",
    "    scores = [4, 3, 3, 2, 1]  # Placeholder, her metni okuyup puanla\n",
    "    semantic_scores[model_name] = {\n",
    "        \"documents\": [r[\"document_id\"] for r in results],\n",
    "        \"scores\": scores,\n",
    "        \"average_score\": np.mean(scores)\n",
    "    }\n",
    "\n",
    "# Jaccard Benzerliği\n",
    "def jaccard_similarity(set1, set2):\n",
    "    set1 = set(set1)\n",
    "    set2 = set(set2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "model_names = list(all_results.keys())\n",
    "jaccard_matrix = np.zeros((len(model_names), len(model_names)))\n",
    "for i, j in combinations(range(len(model_names)), 2):\n",
    "    docs_i = [r[\"document_id\"] for r in all_results[model_names[i]]]\n",
    "    docs_j = [r[\"document_id\"] for r in all_results[model_names[j]]]\n",
    "    jaccard_score = jaccard_similarity(docs_i, docs_j)\n",
    "    jaccard_matrix[i, j] = jaccard_score\n",
    "    jaccard_matrix[j, i] = jaccard_score\n",
    "\n",
    "# Köşegenleri 1 yap\n",
    "np.fill_diagonal(jaccard_matrix, 1)\n",
    "\n",
    "# Sonuçları yazdır\n",
    "print(f\"Giriş Metni: {input_text}\\n\")\n",
    "for model_name, results in all_results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"En Benzer 5 Metin:\")\n",
    "    for r in results:\n",
    "        print(f\"ID: {r['document_id']}, Skor: {r['similarity_score']:.4f}, Metin: {r['content']}\")\n",
    "    print(f\"Anlamsal Skorlar: {semantic_scores[model_name]['scores']}, Ortalama: {semantic_scores[model_name]['average_score']:.2f}\\n\")\n",
    "\n",
    "print(\"Jaccard Benzerlik Matrisi:\")\n",
    "print(pd.DataFrame(jaccard_matrix, index=model_names, columns=model_names).round(2))\n",
    "\n",
    "# Sonuçları CSV'ye kaydet\n",
    "results_df = []\n",
    "for model_name, results in all_results.items():\n",
    "    for r in results:\n",
    "        results_df.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Document_ID\": r[\"document_id\"],\n",
    "            \"Content\": r[\"content\"],\n",
    "            \"Similarity_Score\": r[\"similarity_score\"],\n",
    "            \"Semantic_Score\": semantic_scores[model_name][\"scores\"][results.index(r)]\n",
    "        })\n",
    "pd.DataFrame(results_df).to_csv(\"similarity_results.csv\", index=False)\n",
    "pd.DataFrame(jaccard_matrix, index=model_names, columns=model_names).to_csv(\"jaccard_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6549d678-e42e-41ae-80ff-f6ba171cd552",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
